{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from genai import Client, Credentials\n",
    "\n",
    "genai_key = environ.get('GENAI_KEY', '')\n",
    "genai_url = environ.get('GENAI_API', '')\n",
    "credentials = Credentials(api_key=genai_key, api_endpoint=genai_url)\n",
    "client = Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in client.model.list(limit=100).results:\n",
    "    print(model.model_dump(include=[\"name\", \"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai.schema import (\n",
    "    TextGenerationParameters,\n",
    "    TextGenerationReturnOptions,\n",
    ")\n",
    "\n",
    "for response in client.text.generation.create(\n",
    "    model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\",\n",
    "    inputs=[\"What is a molecule?\", \"What is NLP?\"],\n",
    "    parameters=TextGenerationParameters(\n",
    "        max_new_tokens=150,\n",
    "        min_new_tokens=20,\n",
    "        return_options=TextGenerationReturnOptions(input_text=True),\n",
    "    ),\n",
    "):\n",
    "    result = response.results[0]\n",
    "    print(f\"Input Text: {result.input_text}\")\n",
    "    print(f\"Generated Text: {result.generated_text}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ibm.github.io/ibm-generative-ai/v2.3.0/rst_source/examples.extensions.langchain.langchain_chat_stream.html\n",
    "\n",
    "import pprint\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessageChunk, BaseMessage\n",
    "from genai.extensions.langchain import LangChainChatInterface\n",
    "from genai.schema import DecodingMethod, TextGenerationParameters\n",
    "\n",
    "llm = LangChainChatInterface(\n",
    "    model_id=\"mistralai/mistral-7b-instruct-v0-2\",\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "prompt = \"Describe what is Python in one sentence.\"\n",
    "print(f\"Request: {prompt}\")\n",
    "final: BaseMessage = BaseMessageChunk(content=\"\", type=\"\") # dummy chunk\n",
    "first = True\n",
    "for chunk in llm.stream(\n",
    "    input=[\n",
    "        SystemMessage(\n",
    "            content=\"\"\"You are a helpful, respectful and honest assistant.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature. If a question does not make\n",
    "any sense, or is not factually coherent, explain why instead of answering something incorrectly.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\",\n",
    "        ),\n",
    "        HumanMessage(content=prompt),\n",
    "    ],\n",
    "):\n",
    "    if first:\n",
    "        first = False\n",
    "        pprint.pprint(chunk.response_metadata)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    #info = chunk.generation_info\n",
    "    final = chunk\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "pprint.pprint(final.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    TextGenerationParameters,\n",
    ")\n",
    "\n",
    "llm = LangChainInterface(\n",
    "    model_id=\"mistralai/mistral-7b-instruct-v0-2\",\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.SAMPLE,\n",
    "        max_new_tokens=500,\n",
    "        min_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "prompt = \"Tell me a knock knock joke.\"\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
